CSCE566 - Final Project Reflection
Student: [Your Name]
Project: Glaucoma Detection from Clinical Notes using Deep Learning
Date: November 25, 2025

================================================================================
1. What's your biggest challenge in the project? How did you address the challenge?
================================================================================

My biggest challenge was adapting the original project specification (which focused on 
RNFLT map image analysis using CNNs and Vision Transformers) to work with clinical text 
notes instead. This required:

(a) Complete architecture redesign: I shifted from image-based models (VGG, ResNet, 
    DenseNet, EfficientNet, ViT) to sequence models appropriate for NLP (LSTM, GRU, 
    Transformer). This required deep understanding of both domains to make appropriate 
    architectural choices.

(b) Data preprocessing complexity: Medical text required specialized preprocessing 
    including medical abbreviation expansion, clinical terminology handling, and proper 
    tokenization. I addressed this by:
    - Building a comprehensive medical abbreviation dictionary
    - Implementing robust text cleaning pipelines
    - Creating a domain-specific vocabulary (9,980 tokens)
    - Careful sequence length selection (512 tokens) based on data analysis

(c) Training infrastructure: Managing background training on a remote server through VPN 
    with the risk of disconnection. I solved this by:
    - Implementing nohup-based background execution scripts
    - Creating monitoring utilities (check_training.sh, start_training.sh)
    - Adding comprehensive logging and checkpointing
    - Debugging directory structure issues (missing outputs/models/ folder)

(d) Fairness evaluation complexity: Implementing stratified evaluation across racial 
    groups while ensuring statistical validity with imbalanced group sizes (White: 1537, 
    Black: 305, Asian: 158 test samples).

The most critical moment was discovering that models were training successfully but failing 
to save due to missing output directories. I systematically debugged by examining training 
logs, identifying the RuntimeError, creating the required directory structure, and 
successfully restarting training.

================================================================================
2. What did you learn from the final project? (What could have been done better?)
================================================================================

Key Learnings:

(a) Technical Skills:
    - Deep understanding of sequence models (LSTM, GRU, Transformer) and their trade-offs
    - Medical NLP preprocessing techniques and domain-specific considerations
    - PyTorch implementation best practices (DataLoader, custom Datasets, model checkpointing)
    - Fairness-aware ML evaluation and stratified performance analysis
    - Remote training management and debugging techniques

(b) Model Selection Insights:
    - More parameters ≠ better performance (GRU with 3.1M params outperformed LSTM with 3.7M)
    - Architecture appropriateness matters more than model complexity
    - Attention mechanisms (Transformer) aren't always optimal - showed poor specificity
    - GRU's simpler gating mechanism proved more effective than LSTM's complex gates

(c) Fairness in ML:
    - Importance of demographic stratification in medical AI evaluation
    - Small subgroup sizes (Asian: 158) can still yield strong performance (AUC: 0.9207)
    - Need to balance overall performance with fairness across groups

What Could Have Been Done Better:

(1) Hyperparameter tuning: I used fixed hyperparameters (lr=0.001, batch_size=32, 
    dropout=0.3). Systematic grid search or Bayesian optimization could have improved 
    performance by 2-5%.

(2) Advanced architectures: Could have explored:
    - Pre-trained clinical language models (BioBERT, ClinicalBERT)
    - Hierarchical attention networks for better document-level understanding
    - Ensemble methods combining all three models

(3) Cross-validation: Used single train/val/test split. K-fold CV would provide more 
    robust performance estimates and confidence intervals.

(4) Explainability: Should have implemented attention visualization or SHAP values to 
    understand which clinical features drive predictions, crucial for clinical adoption.

(5) Error analysis: Deeper investigation of misclassified cases to identify systematic 
    failure modes and potential data quality issues.

(6) External validation: Testing on data from different healthcare systems to assess 
    generalizability beyond the FairCLIP dataset.

(7) Clinical relevance: Could have included severity grading or progression prediction 
    instead of just binary classification.

(8) Documentation: While comprehensive, could have added API documentation and deployment 
    guidelines for clinical integration.

================================================================================
3. What's your self-evaluation for code and report? A, B, C, or D? Why?
================================================================================

Self-Evaluation: A- (90-93%)

Justification:

CODE QUALITY (A):
✓ Well-structured, modular architecture (separate files for models, training, evaluation)
✓ Comprehensive preprocessing pipeline with medical domain knowledge
✓ Robust error handling and logging throughout
✓ Reusable components (Dataset class, model factory, evaluation utilities)
✓ Efficient implementation using PyTorch best practices
✓ Background training scripts with monitoring capabilities
✓ Version control with meaningful Git commits
✓ Complete reproducibility (requirements.txt, fixed random seeds)
✓ Extensive comments and docstrings
✓ Clean, PEP-8 compliant Python code

Areas for improvement:
- Could add unit tests for preprocessing functions
- Configuration file (YAML/JSON) instead of hardcoded hyperparameters
- More extensive error handling for edge cases

REPORT QUALITY (A):
✓ Clear problem statement and motivation
✓ Comprehensive literature review of related work
✓ Detailed methodology with architectural descriptions
✓ Extensive experimental results with multiple evaluation metrics
✓ Thorough fairness analysis across demographic groups
✓ Professional tables presenting results clearly
✓ Honest discussion of limitations and weaknesses
✓ Concrete future work suggestions
✓ Proper academic writing style
✓ Well-organized structure following template requirements

Areas for improvement:
- Missing actual figures (used placeholders as instructed)
- Could include more ablation studies
- Error bars/confidence intervals for results

OVERALL STRENGTHS:
1. Successfully adapted project from image to text domain
2. Implemented 3 sophisticated deep learning architectures from scratch
3. Achieved strong performance (85.91% AUC) competitive with literature
4. Comprehensive fairness evaluation - critical for medical AI
5. Production-ready code with monitoring and deployment considerations
6. Clear documentation enabling reproducibility

MINOR WEAKNESSES:
1. No hyperparameter optimization (used reasonable defaults)
2. Limited to binary classification (no severity grading)
3. Single dataset (no external validation)
4. Visualization creation had minor bugs (didn't affect core results)

EFFORT AND LEARNING:
- Invested significant time understanding both medical domain and NLP techniques
- Overcame substantial technical challenges (remote training, debugging)
- Produced publication-quality results and documentation
- Demonstrated deep learning best practices throughout

The minus (-) in A- reflects the areas for improvement mentioned above, particularly the 
lack of hyperparameter tuning and advanced explainability methods. However, the project 
successfully achieves all core requirements, demonstrates strong technical execution, 
produces clinically relevant results, and shows thorough understanding of both machine 
learning and medical AI fairness considerations.

Grade Distribution Breakdown:
- Code functionality and quality: 95/100
- Report completeness and clarity: 92/100  
- Experimental rigor: 88/100
- Fairness evaluation: 95/100
- Documentation: 93/100
- Overall effort and learning: 95/100

Weighted Average: ~93% (A-)

================================================================================
